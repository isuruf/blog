.. post:: 2020-10-02
   :tags: conda-forge
   :author: cj

.. role:: raw-html(raw)
   :format: html

The API Territory and Version Number Map
========================================

tl;dr Depending on specific version numbers of underlying libraries may be too inaccurate and cause
headaches as upstream libraries evolve and change.
A more detailed approach is needed.
In this post I outline current and potential work on a path towards a more complete inspection of 
requirements based on APIs and dynamic pinning of libraries.


What Constitutes a Good Version Number
--------------------------------------

Version numbers should constitute a set that has the following properties

1. The set must be unbounded
2. The set must be orderable (maybe)

Of course sets that meet these requirements might not convey a lot of information
about the software they represent other than if two things are equivalent and their comparative ages.
Note that the requirement to be orderable may not be needed, but is generally useful
when considering the idea of an "upgrade" since it provides a clear delineation between
older and newer packages.
In many cases, the structure of the version number provides additional information.
For some projects the version number includes the date of the release, often using `cal ver <https://calver.org/>`_.
Many projects use `semantic versioning <https://semver.org/>`_, which attempts to encode information about the underlying
source code's API in the version number.

Version Numbers and API Pinning
-------------------------------

One of the most important places where version numbers are specified is during the pinning of APIs.
Source code often requires specific APIs from the libraries it uses.
This requires a pin specifying which versions of the underlying libraries can be used.
The package manager then uses these pins to make certain a compatible environment is created.

However, these pins (or even the lack of pins) produce problems.
Firstly, the pins are a one-time, local statement about the current and future, global ecosystem of packages.
For instance a pin of ``scipy`` to the current major version number may not hold up over time,
newer versions of ``scipy`` may break the API while not changing the major version number.
Similarly the lack of pin for ``scipy`` could be false as the API breaks.
Even pins that establish firm upper and lower bounds may be false as new versions of the
pinned library restore the missing API.
These issues are particularly problematic for dependency systems that tie the pins to a
particular version of the source code, requiring a new version to be created to update
the pins.
Conda-Forge is able to avoid some of these issues via 
`repodata patching <https://github.com/conda-forge/conda-forge-repodata-patches-feedstock>`_, dynamically updating
a package's stated requirements.
Overall this process is fraught, as each package depends on different portions of a library's API,
a version bump that breaks one package may leave others unscathed.

A Potential Path Forward
------------------------

All of the above issues are caused by the confusion of 
`the map for the territory <https://en.wikipedia.org/wiki/Map%E2%80%93territory_relation>`_.
The map, in this case the version number of a library, can not accurately represent
the territory, the API itself.
To fix this issue we need a more accurate description of the territory.
Achieving this will not be easy, but I think there is an approach that gets close enough
to limit the number of errors.

We need a programmatic way to check if a particular library, for a particular version, provides
the required API.
I think this can be achieved iteratively, with each step providing additional clarity and difficulty
of implementation.
Note that in the steps below, I'm using python packaging as an example, but I imagine that these
steps are general enough to apply to other languages and ecosystems.

1. Determine which libraries are requirements of the code, this is provided by tools like `depfinder <https://github.com/ericdill/depfinder>`_
   and are starting to be integrated into the Conda-Forge bot systems (although they are still highly 
   experimental and being worked on).
2. Determine if the a version of the library provides the needed modules. This could be accomplished by
   using depfinder to find the imports and use the mapping provided by 
   `libcfgraph <https://github.com/regro/libcfgraph/tree/master/import_maps>`_ between the 
   import names and the versions of packages that ship those imports.
3. Determine if an imported module provides the symbols being imported. This would require a listing of all
   the symbols in a given python module, including top level scoped variables, function names, class names, methods, etc.
4. For callables determine if the used call signature matches the method or function definition.

The `depfinder <https://github.com/ericdill/depfinder>`_ project has made significant advances along this path, providing
an easy to use tool to extract accurate import and package requirement data from source code.
Depfinder even has cases to handle imports that are within code blocks that might make the requirement optional or use the
python standard library.
Future work on depfinder, including using more accurate maps between imports and package names and providing metadata
on package requirements that are collectively exhaustive (for instance imports of ``pyqt4`` vs. ``pyqt5`` in a ``try: except:`` block),
will provide even more accurate information on requirements.

At each one of the above stages we can provide significant value to users, maintainers and source code authors by helping
them to keep their requirements consistent and warning when there are conflicts.
Conda-Forge can update its repodata as new versions of imported libraries are created, to properly represent
if that version is API compatible with it's downstream consumers.
Additionally the tables that list all the symbols and call signatures can be provided to 3rd party consumers that
may want to patch their own metadata or check if a piece of source code is self consistent in its requirements.
This will also help with the loosening of pins, creating more solvable environments for Conda-Forge and other
packaging ecosystems.
Furthermore, as this tooling matures and becomes more accurate it can be incorporated into the Conda-Forge bot systems
to automatically update dependencies during version bumps and repodata patches, helping reduce maintenance burden.

Tools built from the symbol table can also have impacts far beyond Conda-Forge.
For instance, the symbol tables could allow source code authors to have a line by line inspection of their code,
revealing which lines force the use of older or newer versions of dependencies.
This could enable large scale migrations of source code with surgical precision, enabling developers to extract
and re-write the few lines of code preventing the use of a new version of a library.

Caveats
-------

There are some important caveats to this approach that need to be kept in mind.

1. All of this work is aimed at understanding the API of a given library, this approach
   can not provide insight into the code inside of the API, or if changes there impact
   downstream consumers. For instance, version updates that fix bugs and security flaws 
   in library code may not change the API at all. From this tooling's perspective
   there is no reason to upgrade since the API is not different. Of course there is 
   a strong reason to upgrade in this case, since buggy or vulnerable libraries could be a
   huge headache and liability for downstream code and should be removed as quickly as possible.
2. Some features may depend on broader adoption by the community. For instance, this approach
   would benefit greatly from python type hints, since the API could be constrained down to
   the expected types. Such type constraints would provide much more accuracy to the API version
   range as any changes could be detected. However, type hints may not be adopted in the python 
   community at a high enough rate to truly be useful for this application.
3. Source code is fundamentally flexible. There may be knots of code that even this approach could
   not cut through, especially as multiple languages and runtime module loading come into the picture.
   My personal hope would be that the code recognizes when these situations occur, provides its best
   guess of what is going on, and provides sufficient metadata to users so that they understand the
   decreased accuracy of the results. Fundamentally the tooling can only provide very educated guesses
   and context to users, who then need to go figure out what is actually going on inside the code.

Conclusion
----------
Version number based pins are imprecise representations of API compatibility.
More accurate representations based on source code inspection would make the Conda-Forge
ecosystem more robust and flexible while reducing maintenance burden.
Some of the path to achieving this is built, and near future steps can be achieved with 
current tooling and databases.
